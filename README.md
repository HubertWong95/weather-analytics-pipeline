# ğŸŒ¤ï¸â€¯Weatherâ€¯Analyticsâ€¯Pipeline

Endâ€‘toâ€‘end **batch** dataâ€‘engineering project that ingests, transforms, tests, and visualises a 1â€¯millionâ€‘row synthetic weather dataset for 10â€¯US cities.

> ğŸ”— **Live dashboard** (Lookerâ€¯Studio)  
> https://lookerstudio.google.com/reporting/33be14bf-9aff-422c-80d8-7fe0c071c9f9

---

## ğŸ¯ Project Goals

1. Build a reproducible **batch pipeline** on GCP
2. Model the data with a proper **stagingâ€¯â†’â€¯core** dbt layer
3. Validate data quality with **dbt tests**
4. Surface insights in a **twoâ€‘tile dashboard** (categorical & timeâ€‘series)

---

## âš™ï¸ Tech Stack

| Layer          | Tool                                |
| -------------- | ----------------------------------- |
| Cloud          | **Googleâ€¯Cloudâ€¯Platform**           |
| Dataâ€¯Lake      | **Googleâ€¯Cloudâ€¯Storage (GCS)**      |
| Orchestration  | **Apacheâ€¯Airflow** (Docker Compose) |
| Warehouse      | **BigQuery**                        |
| Transformation | **dbt** (`dbt-bigquery`)            |
| Tests          | **dbt_utils** package               |
| BI             | **Lookerâ€¯Studio**                   |
| Code           | PythonÂ 3.8, Docker                  |

---

## ğŸ” Pipeline Flow

CSV â†’ GCS (raw) â†’ Airflow DAG â†’ BigQuery (weather_data) â†’ dbt (staging â†’ core models) â†’ LookerÂ Studio dashboard

---

## ğŸ§ª Data Quality Tests

| Model                     | Column(s)                                                              | Tests                                      |
| ------------------------- | ---------------------------------------------------------------------- | ------------------------------------------ |
| `stg_weather_data`        | `location`, `date_time`                                                | `not_null`                                 |
|                           | `temperature_c`                                                        | `not_null`, `accepted_rangeâ€¯(-50â€¯â†’â€¯60â€¯Â°C)` |
| `avg_weather_by_location` | `location`                                                             | `not_null`, `unique`                       |
|                           | `avg_temp_c`, `avg_humidity`,<br>`avg_precipitation`, `avg_wind_speed` | `accepted_range` checks (realistic limits) |
| `daily_avg_temperature`   | `day`                                                                  | `not_null`, `unique`                       |
|                           | `avg_temp_c`                                                           | `accepted_rangeâ€¯(-50â€¯â†’â€¯60â€¯Â°C)`             |

All tests pass (`dbt build` âœ **PASSâ€¯13 / FAILâ€¯0**).

---

## ğŸ“Š Dashboard Tiles

| Tile           | Description                              | Source Table              |
| -------------- | ---------------------------------------- | ------------------------- |
| **Bar chart**  | _AverageÂ Temperature by City_            | `avg_weather_by_location` |
| **Line chart** | _DailyÂ AverageÂ Temperature (AllÂ Cities)_ | `daily_avg_temperature`   |

Dashboard link at top of README.

---

## ğŸš€ QuickÂ StartÂ (Local)

```bash
# 0.  Clone repo & create .env with GCP creds if needed
git clone https://github.com/HubertWong95/weather-analytics-pipeline.git
cd weather-analytics-pipeline

# 1.  Build / start Airflow
make airflow-init
make airflow-up         # http://localhost:8080 (airflow/airflow)

# 2.  Upload data to GCS
make upload-gcs

# 3.  Trigger DAG in Airflow (weather_gcs_to_bq) or wait for schedule

# 4.  Run dbt models & tests
make dbt-build          # (= dbt build)

# 5.  Open Looker Studio and refresh tiles
```
